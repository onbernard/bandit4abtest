% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_epsilon_greedy.R
\name{policy_epsilon_greedy}
\alias{policy_epsilon_greedy}
\title{Epsilon Greedy algorithm}
\usage{
policy_epsilon_greedy(visitor_reward, epsilon = 0.25)
}
\arguments{
\item{visitor_reward}{Dataframe of numeric values}

\item{epsilon}{Numeric value (optional)}
}
\value{
\itemize{ List of elements :
 \item S         : numeric matrix of UCB parameters
 \item choice    : numeric vector of arm choice history
 \item proba     : numeric vector of max UCB history
 \item time      : total computation time
 \item theta_hat : estimated reward expectations
 \item theta     : real reward expectations
 }
}
\description{
Control data in visitor_reward with
 \code{\link{bandit_reward_control}} Stop if something is wrong. Generates a
 matrix to save the results (S). At each iteration play the best arm with a
 probability of 1-epsilon and other arm with probability epsilon. Returns the
 calculation time.
 Returns the choice and probability history, computation time, estimated and
 real reward expectations.

 See also \code{\link{condition_for_epsilon_greedy}},
 \code{\link{generate_matrix_S}}, and \code{\link{play_arm}}.
}
\examples{
## Generates 10000 numbers from 2 binomial  distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame(cbind(K1,K2) )
#Run epsilon Greedy algorithm
epsilon_greedy_alloc  <- policy_epsilon_greedy(visitor_reward,epsilon  = 0.25)
epsilon_greedy_alloc$S
barplot(table(epsilon_greedy_alloc$choice),main = "Histogram of choices",
xlab="arm")
epsilon_greedy_alloc$time
epsilon_greedy_alloc$theta_hat
epsilon_greedy_alloc$theta

}
