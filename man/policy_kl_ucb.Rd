% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_kl_ucb.R
\name{policy_kl_ucb}
\alias{policy_kl_ucb}
\title{KL-UCB algorithm}
\usage{
policy_kl_ucb(visitor_reward, precision = 1e-06)
}
\arguments{
\item{visitor_reward}{Dataframe of integer or numeric values}

\item{precision}{Numeric value. Bisection method precision (optional)}
}
\value{
\itemize{ List of element:
 \item S         : Means and trials matrix
 \item choice    : Choice history vector
 \item proba     : Max probability history vector
 \item time      : Computation time
 \item theta_hat : Final estimated reward expectation of each arm
 \item theta     : Actual reward expectation of each arm}
}
\description{
The Kullback-Leibler Upper Confidence Bound (KL-UCB) algorithm is
 used in stochastic bandit with finitely many arms problems.

 When processing a dataframe of reward representing a bandit, the function
 keeps track of each arm estimated reward expectation and number of trials.
 These are returned at the end of the computation in addition to the arm
 played and its associated probability at each iteration, the actual reward
 expectations and the computation time.

 See also \code{\link{condition_for_klucb}}, \code{\link{kl_bernoulli}},
 \code{\link{generate_matrix_S}}, and \code{\link{play_arm}}.

 Reward input is checked for correct dimensions and values. See
 \code{\link{bandit_reward_control}}.
}
\examples{
## Generates 1000 numbers from 2 uniform distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame( cbind(K1,K2) )
klucb_alloc <- policy_kl_ucb(visitor_reward)
klucb_alloc$S
klucb_alloc$time
klucb_alloc$theta
klucb_alloc$theta_hat

}
