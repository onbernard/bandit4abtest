% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_thompson_sampling.R
\name{policy_thompson_sampling}
\alias{policy_thompson_sampling}
\title{Thompson Sampling algorithm}
\usage{
policy_thompson_sampling(visitor_reward, alpha = 1, beta = 1)
}
\arguments{
\item{visitor_reward}{Dataframe of numeric values}

\item{alpha}{Numeric value (optional)}

\item{beta}{Numeric value (optional)}
}
\value{
\itemize{ List of elements :
 \item S         : numeric matrix of UCB parameters
 \item choice    : numeric vector of arm choice history
 \item proba     : numeric vector of max UCB history
 \item time      : total computation time
 \item theta_hat : estimated reward expectations
 \item theta     : real reward expectations
 }
}
\description{
A Thompson sampling (TS) bandit strategy implemented by sampling,
 in each round, averages from a posterior distribution
 \code{\link{condition_for_thompson_sampling}}, and choosing the action that
 maximizes the expected reward given the sampled average. Conceptually, this
 means that the player instantiates their beliefs randomly in each round, and
 then acts optimally according to them.
 Control data in visitor_reward with \code{\link{bandit_reward_control}} and
 \code{\link{control_binary}}.
 Generates a matrix to save the results (S).
 \itemize{ At each iteration
 \item Sample an averages from a posterior in S for each arm (beta
 distribution with alpha and beta parameters)
 \item Choose the arm with the highest average
 \item Receives a reward in visitor_reward for the arm and associated
 iteration
 \item Updates the results matrix S. }

 Returns the choice and probability history, computation time, estimated and
 real reward expectations.

 See also \code{\link{condition_for_thompson_sampling}},
 \code{\link{generate_matrix_S}}, and \code{\link{play_arm}}.
}
\examples{
## Generates 1000 numbers from 2 uniform distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame( cbind(K1,K2) )
policy_thompson_sampling(visitor_reward)

}
