% Generated by roxygen2: do not edit by hand
% Please edit documentation in R/policy_thompson_sampling.R
\name{policy_thompson_sampling}
\alias{policy_thompson_sampling}
\title{Thompson Sampling algorithm}
\usage{
policy_thompson_sampling(visitor_reward, alpha = 1, beta = 1)
}
\arguments{
\item{visitor_reward}{Dataframe of numeric values}

\item{alpha}{Numeric value (optional)}

\item{beta}{Numeric value (optional)}
}
\value{
\itemize{ List of element:
 \item S         : Means and trials matrix
 \item choice    : Choice history vector
 \item proba     : Max probability history vector
 \item time      : Computation time
 \item theta_hat : Final estimated reward expectation of each arm
 \item theta     : Actual reward expectation of each arm}
}
\description{
A Thompson sampling (TS) bandit strategy implemented by sampling,
 in each round, averages from a posterior distribution and choosing the
 action that maximizes the expected reward given the sampled average.
 Conceptually, this means that the player instantiates their beliefs randomly
 in each round, and then acts optimally according to them.

 \itemize{ At each iteration
 \item Sample an averages from a prior for each arm (beta
 distribution with alpha and beta parameters)
 \item Choose the arm with the highest average
 \item Receives a reward in visitor_reward for the arm and associated
 iteration
 \item Update the prior according to the reward. }

 The function keeps track of each arm estimated reward expectation and number
 of trials. These are returned at the end of the computation in addition to
 the arm played and at each iteration, the actual reward expectations and the
 computation time.

 See also \code{\link{condition_for_thompson_sampling}},
 \code{\link{generate_matrix_S}}, and \code{\link{play_arm}}.

 Reward input is checked for correct dimensions and values. These must be
 binary (either numeric or integer ones and zeros) ! See
 \code{\link{bandit_reward_control}} and \code{\link{control_binary}}.
}
\examples{
## Generates 1000 numbers from 2 uniform distributions
set.seed(4434)
K1 <- rbinom(1000, 1, 0.6)
K2 <- rbinom(1000, 1, 0.7)
## Define a dataframe of rewards
visitor_reward <- as.data.frame( cbind(K1,K2) )
policy_thompson_sampling(visitor_reward)

}
